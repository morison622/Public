#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Aug 22 16:14:52 2025

@author: morison
"""

# -*- coding: utf-8 -*-
"""
å®Œå…¨æœ¬åœ°åŒ–çš„å‹•æ…‹æ–‡æœ¬åˆ‡å¡Šç³»çµ±
- ä½¿ç”¨æœ¬åœ° Ollama LLM 
- å¯é¸çš„æœ¬åœ°åµŒå…¥æ¨¡å‹æˆ–ç°¡å–®ç›¸ä¼¼åº¦ç®—æ³•
- ç„¡éœ€ç¶²è·¯é€£æ¥

Modified from original DynamicChunking_HC_v2.py
Author: morison.su (Modified for local execution)
"""

import numpy as np
import ollama
import os
import json
from pathlib import Path
import re
from collections import Counter
import math

# --- æœ¬åœ°åŒ–é…ç½® ---
OLLAMA_MODEL = "llama3.1:latest"
LOCAL_MODE = True  # è¨­ç‚º True å•Ÿç”¨å®Œå…¨æœ¬åœ°æ¨¡å¼

def setup_offline_environment():
    """è¨­ç½®å®Œå…¨é›¢ç·šç’°å¢ƒ"""
    os.environ['TRANSFORMERS_OFFLINE'] = '1'
    os.environ['HF_DATASETS_OFFLINE'] = '1'
    os.environ['OLLAMA_HOST'] = 'localhost:11434'
    print("ğŸ  å·²è¨­ç½®å®Œå…¨æœ¬åœ°åŒ–ç’°å¢ƒ")

def test_ollama_connection():
    """æ¸¬è©¦æœ¬åœ°Ollamaé€£æ¥"""
    try:
        print("ğŸ” æ¸¬è©¦æœ¬åœ° Ollama é€£æ¥...")
        models_response = ollama.list()
        
        model_names = []
        if hasattr(models_response, 'models'):
            for model in models_response.models:
                if hasattr(model, 'model'):
                    model_names.append(model.model)
                elif hasattr(model, 'name'):
                    model_names.append(model.name)
        elif isinstance(models_response, dict) and 'models' in models_response:
            for model in models_response['models']:
                if isinstance(model, dict) and 'name' in model:
                    model_names.append(model['name'])
                elif isinstance(model, str):
                    model_names.append(model)
        
        if model_names:
            print(f"âœ… Ollama é€£æ¥æˆåŠŸï¼å¯ç”¨æ¨¡å‹: {model_names}")
            
            # æª¢æŸ¥ Llama 3.1 æ˜¯å¦å­˜åœ¨
            llama_models = [m for m in model_names if 'llama3.1' in m.lower() or OLLAMA_MODEL in model_names]
            if llama_models or OLLAMA_MODEL in model_names:
                print(f"âœ… æ‰¾åˆ°ç›®æ¨™æ¨¡å‹")
                return True, model_names
            else:
                print(f"âš ï¸ æœªæ‰¾åˆ° {OLLAMA_MODEL}ï¼Œå°‡ä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨æ¨¡å‹")
                return True, model_names
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹")
            return False, []
        
    except Exception as e:
        print(f"âŒ Ollama é€£æ¥å¤±æ•—: {e}")
        print("ğŸ’¡ è§£æ±ºæ–¹æ¡ˆ:")
        print("   1. å•Ÿå‹• Ollama: ollama serve")
        print("   2. ä¸‹è¼‰æ¨¡å‹: ollama pull llama3.1")
        return False, []

def call_ollama_local(prompt, model=OLLAMA_MODEL, max_retries=3):
    """èª¿ç”¨æœ¬åœ°Ollamaï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
    for attempt in range(max_retries):
        try:
            # å¦‚æœæŒ‡å®šæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨æ¨¡å‹
            actual_model = model
            if available_models and model not in available_models:
                actual_model = available_models[0]
                if attempt == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡å˜—è©¦æ™‚é¡¯ç¤ºè­¦å‘Š
                    print(f"âš ï¸ æ¨¡å‹ '{model}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨ '{actual_model}'")
            
            print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ '{actual_model}' è™•ç†... (å˜—è©¦ {attempt + 1}/{max_retries})")
            
            response = ollama.generate(
                model=actual_model,
                prompt=prompt,
                options={
                    'num_ctx': 4096,
                    'temperature': 0.7,
                    'num_predict': 2000,
                    'top_p': 0.9,
                    'repeat_penalty': 1.1
                }
            )
            
            if 'response' in response and response['response'].strip():
                print("âœ… æœ¬åœ° LLM è™•ç†å®Œæˆ")
                return response['response'].strip()
            else:
                print(f"âš ï¸ å˜—è©¦ {attempt + 1} - ç²å¾—ç©ºå›æ‡‰")
                continue
                
        except Exception as e:
            print(f"âŒ å˜—è©¦ {attempt + 1} å¤±æ•—: {e}")
            if attempt == max_retries - 1:
                print("âŒ æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†")
                return None
            print("ğŸ”„ æ­£åœ¨é‡è©¦...")
    
    return None

# --- æœ¬åœ°åµŒå…¥æ¨¡å‹è¼‰å…¥ ---
def load_local_embedding_model():
    """å˜—è©¦è¼‰å…¥æœ¬åœ°åµŒå…¥æ¨¡å‹"""
    if not LOCAL_MODE:
        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… SentenceTransformer æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            return model, True
        except Exception as e:
            print(f"âš ï¸ SentenceTransformer è¼‰å…¥å¤±æ•—: {e}")
            print("ğŸ”„ åˆ‡æ›åˆ°ç°¡å–®ç›¸ä¼¼åº¦æ¨¡å¼")
            return None, False
    else:
        print("ğŸ  æœ¬åœ°æ¨¡å¼ï¼šä½¿ç”¨ç°¡å–®æ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³•")
        return None, False

# --- ç°¡å–®æ–‡æœ¬ç›¸ä¼¼åº¦ç®—æ³• ---
def simple_cosine_similarity(text1, text2):
    """åŸºæ–¼è©é »çš„ç°¡å–®é¤˜å¼¦ç›¸ä¼¼åº¦"""
    def get_word_vector(text):
        words = re.findall(r'\w+', text.lower())
        return Counter(words)
    
    vec1 = get_word_vector(text1)
    vec2 = get_word_vector(text2)
    
    # å–å¾—æ‰€æœ‰ä¸é‡è¤‡çš„è©
    all_words = set(vec1.keys()) | set(vec2.keys())
    
    if not all_words:
        return 0.0
    
    # å»ºç«‹å‘é‡
    v1 = np.array([vec1.get(word, 0) for word in all_words])
    v2 = np.array([vec2.get(word, 0) for word in all_words])
    
    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def get_local_embeddings(texts):
    """ä½¿ç”¨ç°¡å–®æ–¹æ³•ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
    if embedding_model_loaded and embedding_model:
        # ä½¿ç”¨çœŸå¯¦çš„åµŒå…¥æ¨¡å‹
        return embedding_model.encode(texts, convert_to_numpy=True)
    else:
        # ä½¿ç”¨ç°¡å–®çš„è©é »å‘é‡
        embeddings = []
        all_texts_words = []
        
        # æ”¶é›†æ‰€æœ‰è©å½™
        for text in texts:
            words = re.findall(r'\w+', text.lower())
            all_texts_words.extend(words)
        
        vocab = list(set(all_texts_words))
        vocab_size = len(vocab)
        
        if vocab_size == 0:
            return np.zeros((len(texts), 1))
        
        # ç‚ºæ¯å€‹æ–‡æœ¬å»ºç«‹å‘é‡
        for text in texts:
            word_counts = Counter(re.findall(r'\w+', text.lower()))
            vector = np.array([word_counts.get(word, 0) for word in vocab])
            embeddings.append(vector)
        
        return np.array(embeddings)

def simple_hierarchical_clustering(similarity_matrix, threshold):
    """ç°¡å–®çš„éšå±¤å¼èšé¡å¯¦ç¾"""
    n = similarity_matrix.shape[0]
    clusters = [[i] for i in range(n)]
    
    while len(clusters) > 1:
        max_sim = -1
        merge_i, merge_j = -1, -1
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å…©å€‹èšé¡
        for i in range(len(clusters)):
            for j in range(i + 1, len(clusters)):
                # è¨ˆç®—èšé¡é–“çš„å¹³å‡ç›¸ä¼¼åº¦
                sim_sum = 0
                count = 0
                for idx1 in clusters[i]:
                    for idx2 in clusters[j]:
                        sim_sum += similarity_matrix[idx1][idx2]
                        count += 1
                
                if count > 0:
                    avg_sim = sim_sum / count
                    if avg_sim > max_sim:
                        max_sim = avg_sim
                        merge_i, merge_j = i, j
        
        # å¦‚æœæœ€å¤§ç›¸ä¼¼åº¦ä½æ–¼é–¾å€¼ï¼Œåœæ­¢èšé¡
        if max_sim < threshold:
            break
        
        # åˆä½µå…©å€‹æœ€ç›¸ä¼¼çš„èšé¡
        if merge_i != -1 and merge_j != -1:
            clusters[merge_i].extend(clusters[merge_j])
            clusters.pop(merge_j)
    
    # ç”¢ç”Ÿèšé¡æ¨™ç±¤
    labels = [0] * n
    for cluster_id, cluster in enumerate(clusters):
        for idx in cluster:
            labels[idx] = cluster_id
    
    return labels

# --- åŸºç¤åˆ‡å¡Šå‡½æ•¸ ---
def chunk_text_with_overlap(text: str, chunk_size: int, overlap_size: int) -> list[dict]:
    """åŸºç¤é‡ç–Šåˆ‡å¡Š"""
    chunks = []
    text_length = len(text)
    start_index = 0
    chunk_id_counter = 0

    while start_index < text_length:
        end_index = min(start_index + chunk_size, text_length)
        chunk_content = text[start_index:end_index]
        chunks.append({
            "id": f"base_chunk_{chunk_id_counter}",
            "content": chunk_content
        })
        chunk_id_counter += 1

        if end_index == text_length:
            break

        start_index += (chunk_size - overlap_size)
        start_index = min(start_index, text_length - 1)
        
    return chunks

# --- æœ¬åœ°èšé¡å‡½æ•¸ ---
def aggregate_chunks_by_similarity_local(
    base_chunks: list[dict],
    similarity_threshold: float
) -> list[dict]:
    """ä½¿ç”¨æœ¬åœ°ç®—æ³•é€²è¡Œç›¸ä¼¼åº¦èšé¡"""
    if not base_chunks:
        return []
    
    print("ğŸ” é–‹å§‹æœ¬åœ°èªç¾©ç›¸ä¼¼åº¦åˆ†æ...")
    
    base_chunk_contents = [chunk['content'] for chunk in base_chunks]
    
    # ç”ŸæˆåµŒå…¥å‘é‡
    print("ğŸ“Š ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡...")
    embeddings = get_local_embeddings(base_chunk_contents)
    
    # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£
    print("ğŸ§® è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£...")
    n = len(base_chunks)
    similarity_matrix = np.zeros((n, n))
    
    if embedding_model_loaded and embedding_model:
        # ä½¿ç”¨çœŸå¯¦åµŒå…¥çš„é¤˜å¼¦ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarity_matrix = cosine_similarity(embeddings)
    else:
        # ä½¿ç”¨ç°¡å–®ç›¸ä¼¼åº¦
        for i in range(n):
            for j in range(n):
                if i != j:
                    sim = simple_cosine_similarity(base_chunk_contents[i], base_chunk_contents[j])
                    similarity_matrix[i][j] = sim
                else:
                    similarity_matrix[i][j] = 1.0
    
    # åŸ·è¡Œèšé¡
    print("ğŸ”— åŸ·è¡Œéšå±¤å¼èšé¡...")
    cluster_labels = simple_hierarchical_clustering(similarity_matrix, similarity_threshold)
    
    # å°‡åŸºç¤å€å¡ŠæŒ‰èšé¡æ¨™ç±¤åˆ†çµ„
    clusters = {}
    for i, label in enumerate(cluster_labels):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(base_chunks[i])

    aggregated_chunks = []
    super_chunk_id_counter = 0

    # åªè™•ç†åŒ…å«å¤šå€‹å€å¡Šçš„èšé¡
    for label, chunk_group in clusters.items():
        if len(chunk_group) > 1:
            content = "".join([chunk['content'] for chunk in chunk_group])
            original_ids = [chunk['id'] for chunk in chunk_group]
            
            aggregated_chunks.append({
                "id": f"super_chunk_{super_chunk_id_counter}",
                "content": content,
                "original_chunk_ids": original_ids
            })
            super_chunk_id_counter += 1
    
    print(f"âœ… èšé¡å®Œæˆï¼Œç”Ÿæˆ {len(aggregated_chunks)} å€‹èšåˆå¡Š")
    return aggregated_chunks

# --- æœ¬åœ°æ–‡æœ¬ç²¾ç…‰å‡½æ•¸ ---
def refine_super_chunk_local(super_chunk_content: str) -> str:
    """ä½¿ç”¨æœ¬åœ° Ollama ç²¾ç…‰ Super Chunk å…§å®¹"""
    
    prompt = f"""è«‹ç²¾ç…‰ä»¥ä¸‹ä¸­æ–‡æ–‡æœ¬ï¼Œè¦æ±‚ï¼š
1. å»é™¤é‡è¤‡å’Œå†—é¤˜å…§å®¹
2. ä¿æŒæ‰€æœ‰é—œéµè³‡è¨Š
3. ä½¿æ–‡æœ¬æ›´æµæš¢æ˜“è®€
4. ç¶­æŒåŸæ–‡çš„æ„æ€å’Œèªå¢ƒ

åŸå§‹æ–‡æœ¬ï¼š
{super_chunk_content}

ç²¾ç…‰å¾Œçš„æ–‡æœ¬ï¼š"""
    
    try:
        refined_text = call_ollama_local(prompt)
        if refined_text and len(refined_text.strip()) > 20:
            print("âœ… Super Chunk ç²¾ç…‰å®Œæˆ")
            return refined_text
        else:
            print("âš ï¸ LLM å›æ‡‰ç•°å¸¸ï¼Œä¿ç•™åŸæ–‡")
            return super_chunk_content
    except Exception as e:
        print(f"âŒ Super Chunk ç²¾ç…‰å¤±æ•—: {e}")
        return super_chunk_content

# --- ä¸»ç¨‹åº ---
def main():
    """ä¸»åŸ·è¡Œç¨‹åº"""
    
    # è¨­ç½®ç’°å¢ƒ
    setup_offline_environment()
    
    # æ¸¬è©¦ Ollama é€£æ¥
    global ollama_connected, available_models
    ollama_connected, available_models = test_ollama_connection()
    
    if not ollama_connected:
        print("âŒ ç„¡æ³•é€£æ¥åˆ°æœ¬åœ° Ollamaï¼Œç¨‹åºçµ‚æ­¢")
        return
    
    # è¼‰å…¥åµŒå…¥æ¨¡å‹
    global embedding_model, embedding_model_loaded
    embedding_model, embedding_model_loaded = load_local_embedding_model()
    
    # æ¸¬è©¦æ–‡æœ¬
    long_chinese_text = """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ­£åœ¨è¿…é€Ÿæ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚å¾æ™ºèƒ½æ‰‹æ©Ÿä¸­çš„èªéŸ³åŠ©æ‰‹åˆ°è‡ªå‹•é§•é§›æ±½è»Šï¼ŒAI çš„æ‡‰ç”¨ç„¡è™•ä¸åœ¨ã€‚AI çš„æ ¸å¿ƒç›®æ¨™æ˜¯ä½¿æ©Ÿå™¨èƒ½å¤ åƒäººé¡ä¸€æ¨£æ€è€ƒã€å­¸ç¿’å’Œè§£æ±ºå•é¡Œã€‚

æ©Ÿå™¨å­¸ç¿’æ˜¯AIçš„ä¸€å€‹é‡è¦åˆ†æ”¯ï¼Œå®ƒä½¿è¨ˆç®—æ©Ÿç³»çµ±èƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’è€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„å­é ˜åŸŸï¼Œå®ƒåˆ©ç”¨äººå·¥ç¥ç¶“ç¶²çµ¡ï¼Œç‰¹åˆ¥æ˜¯å¤šå±¤æ¬¡ç¶²çµ¡ï¼Œä¾†è™•ç†è¤‡é›œæ¨¡å¼è­˜åˆ¥ä»»å‹™ï¼Œä¾‹å¦‚åœ–åƒè­˜åˆ¥å’Œè‡ªç„¶èªè¨€è™•ç†ã€‚

è‡ªç„¶èªè¨€è™•ç†ï¼ˆNLPï¼‰æ˜¯AIçš„å¦ä¸€å€‹é—œéµé ˜åŸŸï¼Œå®ƒå°ˆæ³¨æ–¼è®“è¨ˆç®—æ©Ÿç†è§£ã€è§£é‡‹å’Œç”Ÿæˆäººé¡èªè¨€ã€‚èªéŸ³è­˜åˆ¥ã€æ©Ÿå™¨ç¿»è­¯å’Œæƒ…æ„Ÿåˆ†æéƒ½æ˜¯NLPçš„æ‡‰ç”¨ã€‚é€éNLPï¼Œæˆ‘å€‘å¯ä»¥èˆ‡è¨ˆç®—æ©Ÿé€²è¡Œæ›´è‡ªç„¶çš„äº’å‹•ã€‚

é›–ç„¶AIå¸¶ä¾†äº†å·¨å¤§çš„æ½›åŠ›ï¼Œä½†ä¹Ÿä¼´éš¨è‘—ä¸€äº›æŒ‘æˆ°ï¼Œä¾‹å¦‚å€«ç†å•é¡Œã€éš±ç§å•é¡Œä»¥åŠå°å°±æ¥­å¸‚å ´çš„å½±éŸ¿ã€‚å¦‚ä½•å¹³è¡¡AIçš„ç™¼å±•èˆ‡ç¤¾æœƒè²¬ä»»æ˜¯æˆ‘å€‘éœ€è¦å…±åŒé¢å°çš„èª²é¡Œã€‚æœªä¾†ï¼ŒAIå°‡ç¹¼çºŒæ·±å…¥ç™¼å±•ï¼Œå½±éŸ¿æˆ‘å€‘ç”Ÿæ´»çš„æ–¹æ–¹é¢é¢ã€‚

äººå·¥æ™ºèƒ½æŠ€è¡“çš„å¿«é€Ÿç™¼å±•ç‚ºå„å€‹è¡Œæ¥­å¸¶ä¾†äº†è®Šé©æ€§å½±éŸ¿ã€‚åœ¨é†«ç™‚é ˜åŸŸï¼ŒAIå”åŠ©é†«ç”Ÿé€²è¡Œç–¾ç—…è¨ºæ–·å’Œæ²»ç™‚æ–¹æ¡ˆåˆ¶å®šã€‚åœ¨é‡‘èæ¥­ï¼ŒAIç”¨æ–¼é¢¨éšªè©•ä¼°å’Œè©é¨™æª¢æ¸¬ã€‚æ•™è‚²é ˜åŸŸä¸­ï¼Œå€‹æ€§åŒ–å­¸ç¿’ç³»çµ±èƒ½å¤ æ ¹æ“šå­¸ç”Ÿçš„éœ€æ±‚æä¾›é‡èº«å®šåˆ¶çš„æ•™å­¸å…§å®¹ã€‚

ç„¶è€Œï¼Œéš¨è‘—AIæŠ€è¡“çš„æ™®åŠï¼Œæˆ‘å€‘ä¹Ÿå¿…é ˆé—œæ³¨å…¶å¸¶ä¾†çš„æŒ‘æˆ°ã€‚æ•¸æ“šéš±ç§ä¿è­·ã€ç®—æ³•åè¦‹ã€å°±æ¥­çµæ§‹è®ŠåŒ–ç­‰å•é¡Œéƒ½éœ€è¦æˆ‘å€‘èªçœŸæ€è€ƒå’Œæ‡‰å°ã€‚åªæœ‰åœ¨æŠ€è¡“ç™¼å±•èˆ‡å€«ç†è²¬ä»»ä¹‹é–“æ‰¾åˆ°å¹³è¡¡ï¼ŒAIæ‰èƒ½çœŸæ­£ç‚ºäººé¡ç¤¾æœƒå¸¶ä¾†ç¦ç¥‰ã€‚
"""
    
    print("=" * 60)
    print("ğŸ  å®Œå…¨æœ¬åœ°åŒ–å‹•æ…‹æ–‡æœ¬åˆ‡å¡Šç³»çµ±")
    print("=" * 60)
    print(f"ğŸ“„ åŸå§‹æ–‡æœ¬é•·åº¦: {len(long_chinese_text)} å­—ç¬¦")
    print("-" * 30)
    print(long_chinese_text)
    print("-" * 30)
    
    # è¨­ç½®åƒæ•¸
    CHUNK_SIZE = 60
    OVERLAP_SIZE = 10
    SIMILARITY_THRESHOLD = 0.3
    
    print(f"\nğŸ“‹ è™•ç†åƒæ•¸:")
    print(f"   - å¡Šå¤§å°: {CHUNK_SIZE}")
    print(f"   - é‡ç–Šå¤§å°: {OVERLAP_SIZE}")
    print(f"   - ç›¸ä¼¼åº¦é–¾å€¼: {SIMILARITY_THRESHOLD}")
    
    # æ­¥é©Ÿ 1: åŸºç¤åˆ‡å¡Š
    print(f"\nğŸ”ª æ­¥é©Ÿ 1: åŸºç¤é‡ç–Šåˆ‡å¡Š")
    base_chunks = chunk_text_with_overlap(long_chinese_text, CHUNK_SIZE, OVERLAP_SIZE)
    
    print(f"âœ… ç”Ÿæˆ {len(base_chunks)} å€‹åŸºç¤å¡Š:")
    for i, chunk in enumerate(base_chunks):
        print(f"   ğŸ“¦ {chunk['id']} (é•·åº¦: {len(chunk['content'])})")
        print(f"      å…§å®¹: {chunk['content'][:50]}{'...' if len(chunk['content']) > 50 else ''}")
    
    # æ­¥é©Ÿ 2: èªç¾©èšé¡
    print(f"\nğŸ”— æ­¥é©Ÿ 2: èªç¾©ç›¸ä¼¼åº¦èšé¡")
    super_chunks = aggregate_chunks_by_similarity_local(base_chunks, SIMILARITY_THRESHOLD)
    
    if not super_chunks:
        print("âš ï¸ æœªç”Ÿæˆèšåˆå¡Šï¼Œå¯èƒ½éœ€è¦èª¿æ•´ç›¸ä¼¼åº¦é–¾å€¼")
        print("ğŸ ç¨‹åºçµæŸ")
        return
    
    print(f"âœ… ç”Ÿæˆ {len(super_chunks)} å€‹èšåˆå¡Š:")
    
    # æ­¥é©Ÿ 3: LLM ç²¾ç…‰
    print(f"\nğŸ¤– æ­¥é©Ÿ 3: LLM å…§å®¹ç²¾ç…‰")
    
    for i, super_chunk in enumerate(super_chunks):
        print(f"\nğŸ“¦ è™•ç†èšåˆå¡Š {super_chunk['id']}")
        print(f"   ğŸ“‹ åŒ…å«åŸºç¤å¡Š: {super_chunk['original_chunk_ids']}")
        print(f"   ğŸ“ åŸå§‹é•·åº¦: {len(super_chunk['content'])} å­—ç¬¦")
        print(f"   ğŸ“ åŸå§‹å…§å®¹: {super_chunk['content'][:100]}{'...' if len(super_chunk['content']) > 100 else ''}")
        
        # ä½¿ç”¨ LLM ç²¾ç…‰
        refined_content = refine_super_chunk_local(super_chunk['content'])
        
        print(f"\n   âœ¨ ç²¾ç…‰å¾Œé•·åº¦: {len(refined_content)} å­—ç¬¦")
        print(f"   âœ¨ ç²¾ç…‰å¾Œå…§å®¹:")
        print(f"      {refined_content}")
        print("   " + "=" * 50)
    
    # ç¸½çµ
    print(f"\nğŸ“Š è™•ç†ç¸½çµ:")
    print(f"   ğŸ“„ åŸå§‹æ–‡æœ¬: {len(long_chinese_text)} å­—ç¬¦")
    print(f"   ğŸ“¦ åŸºç¤å¡Šæ•¸: {len(base_chunks)} å€‹")
    print(f"   ğŸ”— èšåˆå¡Šæ•¸: {len(super_chunks)} å€‹")
    print(f"   ğŸ¤– ä½¿ç”¨æ¨¡å‹: {OLLAMA_MODEL}")
    print(f"   ğŸ  é‹è¡Œæ¨¡å¼: {'å®Œå…¨æœ¬åœ°åŒ–' if LOCAL_MODE else 'æ··åˆæ¨¡å¼'}")
    print(f"   ğŸ“Š åµŒå…¥æ–¹æ³•: {'SentenceTransformer' if embedding_model_loaded else 'ç°¡å–®ç›¸ä¼¼åº¦'}")
    
    print(f"\nğŸ‰ å®Œå…¨æœ¬åœ°åŒ–è™•ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()